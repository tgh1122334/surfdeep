library(keras)
max_words <- 1000
batch_size <- 32
epochs <- 5
cat('Loading data...\n')
reuters <- dataset_reuters(num_words = max_words, test_split = 0.2)
x_train <- reuters$train$x
y_train <- reuters$train$y
x_test <- reuters$test$x
y_test <- reuters$test$y
cat(length(x_train), 'train sequences\n')
cat(length(x_test), 'test sequences\n')
num_classes <- max(y_train) + 1
cat(num_classes, '\n')
cat('Vectorizing sequence data...\n')
tokenizer <- text_tokenizer(num_words = max_words)
x_train <- sequences_to_matrix(tokenizer, x_train, mode = 'binary')
x_test <- sequences_to_matrix(tokenizer, x_test, mode = 'binary')
cat('x_train shape:', dim(x_train), '\n')
cat('x_test shape:', dim(x_test), '\n')
cat('Convert class vector to binary class matrix',
'(for use with categorical_crossentropy)\n')
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
cat('y_train shape:', dim(y_train), '\n')
cat('y_test shape:', dim(y_test), '\n')
cat('Building model...\n')
model <- keras_model_sequential()
model %>%
layer_dense(units = 512, input_shape = c(max_words)) %>%
layer_activation(activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes) %>%
layer_activation(activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
verbose = 1,
validation_split = 0.1
)
score <- model %>% evaluate(
x_test, y_test,
batch_size = batch_size,
verbose = 1
)
library(tensorflow)
NUM_CLASSES <- 10L
IMAGE_SIZE <- 28L
IMAGE_PIXELS <- IMAGE_SIZE * IMAGE_SIZE
FLAGS <- flags(
flag_numeric('learning_rate', 0.01, 'Initial learning rate.'),
flag_integer('max_steps', 5000L, 'Number of steps to run trainer.'),
flag_integer('hidden1', 128L, 'Number of units in hidden layer 1.'),
flag_integer('hidden2', 32L, 'Number of units in hidden layer 2.'),
flag_integer('batch_size', 100L, 'Batch size. Must divide evenly into the dataset sizes.'),
flag_string('train_dir', 'MNIST-data', 'Directory to put the training data.'),
flag_boolean('fake_data', FALSE, 'If true, uses fake data for unit testing.')
)
input_data <- tf$contrib$learn$datasets$mnist
inference <- function(images, hidden1_units, hidden2_units) {
# Hidden 1
with(tf$name_scope('hidden1'), {
weights <- tf$Variable(
tf$truncated_normal(shape(IMAGE_PIXELS, hidden1_units),
stddev = 1.0 / sqrt(IMAGE_PIXELS)),
name = 'weights'
)
biases <- tf$Variable(tf$zeros(shape(hidden1_units),
name = 'biases'))
hidden1 <- tf$nn$relu(tf$matmul(images, weights) + biases)
})
# Hidden 2
with(tf$name_scope('hidden2'), {
weights <- tf$Variable(
tf$truncated_normal(shape(hidden1_units, hidden2_units),
stddev = 1.0 / sqrt(hidden1_units)),
name = 'weights')
biases <- tf$Variable(tf$zeros(shape(hidden2_units)),
name = 'biases')
hidden2 <- tf$nn$relu(tf$matmul(hidden1, weights) + biases)
})
# Linear
with(tf$name_scope('softmax_linear'), {
weights <- tf$Variable(
tf$truncated_normal(shape(hidden2_units, NUM_CLASSES),
stddev = 1.0 / sqrt(hidden2_units)),
name = 'weights')
biases <- tf$Variable(tf$zeros(shape(NUM_CLASSES)),
name = 'biases')
logits <- tf$matmul(hidden2, weights) + biases
})
# return logits
logits
}
loss <- function(logits, labels) {
labels <- tf$to_int64(labels)
cross_entropy <- tf$nn$sparse_softmax_cross_entropy_with_logits(
logits = logits, labels = labels, name = 'xentropy')
tf$reduce_mean(cross_entropy, name = 'xentropy_mean')
}
training <- function(loss, learning_rate) {
# Add a scalar summary for the snapshot loss.
tf$summary$scalar(loss$op$name, loss)
# Create the gradient descent optimizer with the given learning rate.
optimizer <- tf$train$GradientDescentOptimizer(learning_rate)
# Create a variable to track the global step.
global_step <- tf$Variable(0L, name = 'global_step', trainable = FALSE)
# Use the optimizer to apply the gradients that minimize the loss
# (and also increment the global step counter) as a single training step.
optimizer$minimize(loss, global_step = global_step)
}
evaluation <- function(logits, labels) {
# For a classifier model, we can use the in_top_k Op.
# It returns a bool tensor with shape [batch_size] that is true for
# the examples where the label is in the top k (here k=1)
# of all logits for that example.
correct <- tf$nn$in_top_k(logits, labels, 1L)
tf$reduce_sum(tf$cast(correct, tf$int32))
}
placeholder_inputs <- function(batch_size) {
# Note that the shapes of the placeholders match the shapes of the full
# image and label tensors, except the first dimension is now batch_size
# rather than the full size of the train or test data sets.
images <- tf$placeholder(tf$float32, shape(batch_size, IMAGE_PIXELS))
labels <- tf$placeholder(tf$int32, shape(batch_size))
# return both placeholders
list(images = images, labels = labels)
}
fill_feed_dict <- function(data_set, images_pl, labels_pl) {
# Create the feed_dict for the placeholders filled with the next
# `batch size` examples.
batch <- data_set$next_batch(FLAGS$batch_size, FLAGS$fake_data)
images_feed <- batch[[1]]
labels_feed <- batch[[2]]
dict(
images_pl = images_feed,
labels_pl = labels_feed
)
}
do_eval <- function(sess,
eval_correct,
images_placeholder,
labels_placeholder,
data_set) {
# And run one epoch of eval.
true_count <- 0  # Counts the number of correct predictions.
steps_per_epoch <- data_set$num_examples %/% FLAGS$batch_size
num_examples <- steps_per_epoch * FLAGS$batch_size
for (step in 1:steps_per_epoch) {
feed_dict <- fill_feed_dict(data_set,
images_placeholder,
labels_placeholder)
true_count <- true_count + sess$run(eval_correct, feed_dict=feed_dict)
}
precision <- true_count / num_examples
cat(sprintf('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f\n',
num_examples, true_count, precision))
}
data_sets <- input_data$read_data_sets(FLAGS$train_dir, FLAGS$fake_data)
with(tf$Graph()$as_default(), {
# Generate placeholders for the images and labels.
placeholders <- placeholder_inputs(FLAGS$batch_size)
# Build a Graph that computes predictions from the inference model.
logits <- inference(placeholders$images, FLAGS$hidden1, FLAGS$hidden2)
# Add to the Graph the Ops for loss calculation.
loss <- loss(logits, placeholders$labels)
# Add to the Graph the Ops that calculate and apply gradients.
train_op <- training(loss, FLAGS$learning_rate)
# Add the Op to compare the logits to the labels during evaluation.
eval_correct <- evaluation(logits, placeholders$labels)
# Build the summary Tensor based on the TF collection of Summaries.
summary <- tf$summary$merge_all()
# Add the variable initializer Op.
init <- tf$global_variables_initializer()
# Create a saver for writing training checkpoints.
saver <- tf$train$Saver()
# Create a session for running Ops on the Graph.
sess <- tf$Session()
# Instantiate a SummaryWriter to output summaries and the Graph.
summary_writer <- tf$summary$FileWriter(FLAGS$train_dir, sess$graph)
# And then after everything is built:
# Run the Op to initialize the variables.
sess$run(init)
# Start the training loop.
for (step in 1:FLAGS$max_steps) {
start_time <- Sys.time()
# Fill a feed dictionary with the actual set of images and labels
# for this particular training step.
feed_dict <- fill_feed_dict(data_sets$train,
placeholders$images,
placeholders$labels)
# Run one step of the model.  The return values are the activations
# from the `train_op` (which is discarded) and the `loss` Op.  To
# inspect the values of your Ops or variables, you may include them
# in the list passed to sess.run() and the value tensors will be
# returned in the tuple from the call.
values <- sess$run(list(train_op, loss), feed_dict = feed_dict)
loss_value <- values[[2]]
duration <- Sys.time() - start_time
# Write the summaries and print an overview fairly often.
if (step %% 100 == 0) {
# Print status to stdout.
cat(sprintf('Step %d: loss = %.2f (%.3f sec)\n',
step, loss_value, duration))
# Update the events file.
summary_str <- sess$run(summary, feed_dict=feed_dict)
summary_writer$add_summary(summary_str, step)
summary_writer$flush()
}
# Save a checkpoint and evaluate the model periodically.
if ((step + 1) %% 1000 == 0 || (step + 1) == FLAGS$max_steps) {
checkpoint_file <- file.path(FLAGS$train_dir, 'checkpoint')
saver$save(sess, checkpoint_file, global_step=step)
# Evaluate against the training set.
cat('Training Data Eval:\n')
do_eval(sess,
eval_correct,
placeholders$images,
placeholders$labels,
data_sets$train)
# Evaluate against the validation set.
cat('Validation Data Eval:\n')
do_eval(sess,
eval_correct,
placeholders$images,
placeholders$labels,
data_sets$validation)
# Evaluate against the test set.
cat('Test Data Eval:\n')
do_eval(sess,
eval_correct,
placeholders$images,
placeholders$labels,
data_sets$test)
}
}
})
source("mnist.R",echo = T)
library(tensorflow)
data_sets <- input_data$read_data_sets(FLAGS$train_dir, FLAGS$fake_data)
library(tensorflow)
NUM_CLASSES <- 10L
IMAGE_SIZE <- 28L
IMAGE_PIXELS <- IMAGE_SIZE * IMAGE_SIZE
input_data <- tf$contrib$learn$datasets$mnist
data_sets <- input_data$read_data_sets(FLAGS$train_dir, FLAGS$fake_data)
FLAGS <- flags(
flag_numeric('learning_rate', 0.01, 'Initial learning rate.'),
flag_integer('max_steps', 5000L, 'Number of steps to run trainer.'),
flag_integer('hidden1', 128L, 'Number of units in hidden layer 1.'),
flag_integer('hidden2', 32L, 'Number of units in hidden layer 2.'),
flag_integer('batch_size', 100L, 'Batch size. Must divide evenly into the dataset sizes.'),
flag_string('train_dir', 'MNIST-data', 'Directory to put the training data.'),
flag_boolean('fake_data', FALSE, 'If true, uses fake data for unit testing.')
)
FLAGS$train_dir
?placeholder_inputs
names(tf)
tf$placeholder
?tf$placeholder
?shape
shape(1)
shape(2)
shape(23)
?with
library(tensorflow)
x_data <- runif(100, min=0, max=1)
y_data <- x_data * 0.1 + 0.3
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
b <- tf$Variable(tf$zeros(shape(1L)))
y <- W * x_data + b
x
w
W
?tf$Variable
tf$Variable
loss <- tf$reduce_mean((y - y_data) ^ 2)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
sess = tf$Session()
sess$run(tf$global_variables_initializer())
for (step in 1:201) {
sess$run(train)
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
sess$run(W)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess$run(train)
sess
sess$run(W)
?shape
f=function()
{}
rm(f)
source("mnist.R")
library(keras)
max_words <- 1000
batch_size <- 32
epochs <- 5
library(keras)
max_words <- 1000
batch_size <- 32
epochs <- 5
cat('Loading data...\n')
reuters <- dataset_reuters(num_words = max_words, test_split = 0.2)
x_train <- reuters$train$x
y_train <- reuters$train$y
x_test <- reuters$test$x
y_test <- reuters$test$y
reuters <- dataset_reuters(num_words = max_words, test_split = 0.2)
library(keras)
max_words <- 1000
batch_size <- 32
epochs <- 5
cat('Loading data...\n')
reuters <- dataset_reuters(num_words = max_words, test_split = 0.2)
x_train <- reuters$train$x
y_train <- reuters$train$y
x_test <- reuters$test$x
y_test <- reuters$test$y
cat(length(x_train), 'train sequences\n')
cat(length(x_test), 'test sequences\n')
num_classes <- max(y_train) + 1
cat(num_classes, '\n')
cat('Vectorizing sequence data...\n')
tokenizer <- text_tokenizer(num_words = max_words)
x_train <- sequences_to_matrix(tokenizer, x_train, mode = 'binary')
x_test <- sequences_to_matrix(tokenizer, x_test, mode = 'binary')
cat('x_train shape:', dim(x_train), '\n')
cat('x_test shape:', dim(x_test), '\n')
cat('Convert class vector to binary class matrix',
'(for use with categorical_crossentropy)\n')
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
cat('y_train shape:', dim(y_train), '\n')
cat('y_test shape:', dim(y_test), '\n')
source("reuters_mlp.R")
dim(x_test)
? layer_conv_2d
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
#layer_dense(units = 512, input_shape = c(max_words)) %>%
layer_activation(activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes) %>%
layer_activation(activation = 'softmax')
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = c(max_words)) %>%
#layer_dense(units = 512, input_shape = c(max_words)) %>%
layer_activation(activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes) %>%
layer_activation(activation = 'softmax')
model <- keras_model_sequential()
model %>%
layer_dense(units = 512, input_shape = c(max_words)) %>%
layer_activation(activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes) %>%
layer_activation(activation = 'softmax')
?dense
?layer_dense
?layer_activation
model <- keras_model_sequential()
model %>%
layer_dense(units = 512, input_shape = c(max_words)) %>%
layer_activation(activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 64, input_shape = 512) %>%
layer_dense(units = num_classes) %>%
layer_activation(activation = 'softmax')
model <- keras_model_sequential()
model %>%
layer_dense(units = 512, input_shape = c(max_words)) %>%
layer_activation(activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 64, input_shape = 512) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes) %>%
layer_activation(activation = 'softmax')
plot(history)
source("reuters_mlp.R")
?source
source("reuters_mlp.R")
score
?layer_activation
source("reuters_mlp.R")
score
score
source("reuters_mlp.R")
source("reuters_mlp.R")
source("reuters_mlp.R")
source("reuters_mlp.R")
source("reuters_mlp.R")
source("reuters_mlp.R")
source("reuters_mlp.R")
source("reuters_mlpold.R")
source("reuters_mlp.R")
plot(history)
source("mnist.R")
