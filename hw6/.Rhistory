source("mnist_fully_connected_feed.R")
source("cifar10_cnn.R")
library(keras)
batch_size <- 32
epochs <- 200
data_augmentation <- TRUE
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
# Feature scale RGB values in test and train inputs
x_train <- cifar10$train$x/255
x_test <- cifar10$test$x/255
y_train <- to_categorical(cifar10$train$y, num_classes = 10)
reticulate::py_config()
devtools::install_github("rstudio/reticulate")
install.packages("devtools")
devtools
devtools::install_github("rstudio/reticulate")
batch_size <- 32
epochs <- 200
data_augmentation <- TRUE
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
?install_keras()
install_keras(method = "conda")
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
library(keras)
batch_size <- 32
epochs <- 200
data_augmentation <- TRUE
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
install_keras(method = "conda",tensorflow = "-gpu")
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
library(keras)
batch_size <- 32
epochs <- 200
data_augmentation <- TRUE
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
install.packages("reticulate")
install.packages("reticulate")
library(reticulate)
use_python("/home/li/anaconda3/bin/python")
library(keras)
batch_size <- 32
epochs <- 200
data_augmentation <- TRUE
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
use_condaenv("pymc_tutorial")
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
library(reticulate)
py_run_string('from six.moves.urllib.request import urlretrieve')
py_run_string("urlretrieve(url='https://www.google.com')")
py_run_string("urlretrieve(url='http://www.google.com')")
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
1+1
library(keras)
batch_size <- 32
epochs <- 200
data_augmentation <- TRUE
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
# input_data
input_data <- tf$contrib$learn$datasets$mnist
library(keras)
# Embedding
max_features = 20000
maxlen = 100
embedding_size = 128
# Convolution
kernel_size = 5
filters = 64
pool_size = 4
# LSTM
lstm_output_size = 70
# Training
batch_size = 30
epochs = 2
# The x data includes integer sequences, each integer is a word
# The y data includes a set of integer labels (0 or 1)
# The num_words argument indicates that only the max_fetures most frequent
# words will be integerized. All other will be ignored.
# See help(dataset_imdb)
imdb <- dataset_imdb(num_words = max_features)
# Keras load all data into a list with the following structure:
str(imdb)
install_keras()
# The x data includes integer sequences, each integer is a word
# The y data includes a set of integer labels (0 or 1)
# The num_words argument indicates that only the max_fetures most frequent
# words will be integerized. All other will be ignored.
# See help(dataset_imdb)
imdb <- dataset_imdb(num_words = max_features)
source("mnist_fully_connected_feed.R")
library(keras)
batch_size <- 32
epochs <- 200
data_augmentation <- TRUE
# See ?dataset_cifar10 for more info
cifar10 <- dataset_cifar10()
# Feature scale RGB values in test and train inputs
x_train <- cifar10$train$x/255
x_test <- cifar10$test$x/255
y_train <- to_categorical(cifar10$train$y, num_classes = 10)
y_test <- to_categorical(cifar10$test$y, num_classes = 10)
# Initialize sequential model
model <- keras_model_sequential()
model %>%
# Start with hidden 2D convolutional layer being fed 32x32 pixel images
layer_conv_2d(
filter = 32, kernel_size = c(3,3), padding = "same",
input_shape = c(32, 32, 3)
) %>%
layer_activation("relu") %>%
# Second hidden layer
layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
# 2 additional hidden 2D convolutional layers
layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = "same") %>%
layer_activation("relu") %>%
layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%
layer_activation("relu") %>%
# Use max pooling once more
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
# Flatten max filtered output into feature vector
# and feed into dense layer
layer_flatten() %>%
layer_dense(512) %>%
layer_activation("relu") %>%
layer_dropout(0.5) %>%
# Outputs from dense layer are projected onto 10 unit output layer
layer_dense(10) %>%
layer_activation("softmax")
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = opt,
metrics = "accuracy"
)
if(!data_augmentation){
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_data = list(x_test, y_test),
shuffle = TRUE
)
} else {
datagen <- image_data_generator(
featurewise_center = TRUE,
featurewise_std_normalization = TRUE,
rotation_range = 20,
width_shift_range = 0.2,
height_shift_range = 0.2,
horizontal_flip = TRUE
)
datagen %>% fit_image_data_generator(x_train)
model %>% fit_generator(
flow_images_from_data(x_train, y_train, datagen, batch_size = batch_size),
steps_per_epoch = as.integer(50000/batch_size),
epochs = epochs,
validation_data = list(x_test, y_test)
)
}
install_keras(tensorflow = "-gpu")
